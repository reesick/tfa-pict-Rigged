# PERSON 1: CORE BACKEND & DATABASE
## STRATEGIC DEVELOPMENT GUIDE (NO CODE)

---

## YOUR MISSION

You are the **Foundation Architect** - the backbone of the entire system. Build the FastAPI application, design the database schema, implement authentication, create REST APIs, set up Docker infrastructure, and establish real-time WebSocket connections. Every other team member depends on your work being solid, scalable, and well-documented. **25% of total project work.**

**Timeline:** Weeks 1-3 (Core foundation), then ongoing API additions  
**Dependencies:** None - you start first  
**Who Depends On You:** Everyone (Person 2, 3, 4 all blocked without your APIs)

---

## WHAT YOU'LL BUILD

1. **FastAPI Application** - Async REST API with automatic documentation
2. **PostgreSQL Database** - Complete schema with 10+ tables, indexes, relationships
3. **Authentication System** - JWT tokens (access + refresh), password hashing
4. **Core APIs** - Transactions, budgets, merchants, user management
5. **WebSocket Server** - Real-time updates for frontend
6. **Docker Stack** - Postgres, Redis, backend containers
7. **Celery Setup** - Background task infrastructure for Person 2
8. **Database Migrations** - Alembic for version control

---

## CRITICAL ARCHITECTURAL DECISIONS

### RESEARCH SEPARATELY: Technology Choices

Before starting, research and decide on:

1. **FastAPI Deployment Strategy**
   - Uvicorn alone vs Gunicorn + Uvicorn workers
   - Number of workers (2 x CPU cores recommended)
   - Production ASGI server configuration

2. **Database Connection Management**
   - asyncpg (async driver) vs psycopg2 (sync)
   - Connection pooling parameters (pool_size, max_overflow)
   - Connection lifecycle management

3. **Project Structure**
   - Feature-based structure (recommended for scalability)
   - Layer-based structure (simpler for smaller apps)
   - Netflix Dispatch pattern vs traditional MVC

4. **Session Management**
   - Database sessions (PostgreSQL table)
   - Redis sessions (faster, ephemeral)
   - JWT-only (stateless, no server-side session)

**Recommendations Based on Research:**
- **ASGI Server:** Gunicorn with 4-8 Uvicorn workers for production
- **DB Driver:** asyncpg (3x faster than psycopg2 for async operations)
- **Structure:** Feature-based (auth/, transactions/, budgets/)
- **Sessions:** Redis for speed, PostgreSQL backup for persistence

---

## PHASE 1: PROJECT INITIALIZATION (Days 1-2)

### What To Do
Set up project structure, install dependencies, configure environment

### How To Do It Efficiently

**Project Structure Decision:**
```
Option A (Feature-Based - RECOMMENDED):
backend/
├── app/
│   ├── auth/           # Authentication module
│   ├── transactions/   # Transaction module
│   ├── budgets/        # Budget module
│   ├── core/           # Shared utilities
│   └── main.py         # App entry point

Option B (Layer-Based):
backend/
├── app/
│   ├── api/            # All routes
│   ├── models/         # All DB models
│   ├── schemas/        # All Pydantic schemas
│   └── services/       # All business logic
```

**Why Feature-Based Wins:**
- Better scalability (modules grow independently)
- Easier to understand (related code together)
- Clearer ownership (Person 2 owns ingestion module)
- Faster development (less file switching)

**Essential Dependencies:**
- **Web Framework:** fastapi==0.104.1, uvicorn[standard]==0.24.0
- **Database:** sqlalchemy==2.0.23, asyncpg==0.29.0, alembic==1.12.1
- **Async Support:** aiofiles==23.2.1, httpx==0.25.2
- **Authentication:** pyjwt==2.8.0, bcrypt==4.1.1, python-jose==3.3.0
- **Validation:** pydantic==2.5.0, pydantic-settings==2.1.0
- **Task Queue:** celery==5.3.4, redis==5.0.1
- **Testing:** pytest==7.4.3, pytest-asyncio==0.21.1
- **Environment:** python-dotenv==1.0.0

**Virtual Environment Setup:**
- Use Python 3.11+ (performance improvements, better typing)
- Create venv, activate, install dependencies
- **Pro Tip:** Use `pip freeze > requirements.txt` after installing

**Environment Configuration Strategy:**
- Create `.env` file for local development
- Use `.env.example` template in repo (no secrets)
- Production: Environment variables via hosting platform
- **Never commit secrets to Git**

**Configuration Values Needed:**
- `DATABASE_URL` - PostgreSQL connection string
- `REDIS_URL` - Redis connection string
- `JWT_SECRET` - Random 64-character hex (use `openssl rand -hex 32`)
- `JWT_ALGORITHM` - HS256 (standard)
- `ACCESS_TOKEN_EXPIRE_MINUTES` - 60 (1 hour)
- `REFRESH_TOKEN_EXPIRE_DAYS` - 30
- `CORS_ORIGINS` - Frontend URLs (comma-separated)

---

## PHASE 2: DATABASE DESIGN & MODELS (Days 3-6)

### What To Do
Design complete database schema, create SQLAlchemy models, set up Alembic migrations

### How To Do It Efficiently

**RESEARCH SEPARATELY: PostgreSQL Optimization**
- GIN indexes for JSONB and array columns
- Composite indexes for multi-column queries
- Partial indexes for filtered queries
- Full-text search with tsvector
- Connection pooling best practices

**Database Design Principles:**
1. **Normalize to 3NF** - Eliminate redundancy
2. **Use UUIDs for primary keys** - Better for distributed systems, Person 3's blockchain needs
3. **JSONB for flexible data** - User preferences, confidence scores
4. **Proper foreign keys** - Maintain referential integrity
5. **Timestamps everywhere** - created_at, updated_at for audit trails

**Core Tables To Create:**

**1. users table**
- **Purpose:** Store user accounts
- **Key Fields:** id (UUID), email (unique), hashed_password, phone, full_name
- **Blockchain:** wallet_addresses (JSONB array)
- **Preferences:** preferences (JSONB) - currency, theme, notifications
- **Status:** is_active (boolean), is_verified (boolean)
- **Timestamps:** created_at, updated_at
- **Indexes:** email (unique), created_at (for pagination)

**2. transactions table**
- **Purpose:** Store all financial transactions
- **Key Fields:** id (UUID), user_id (FK to users), amount (NUMERIC 12,2), date (DATE)
- **Merchant:** merchant_raw (TEXT), merchant_id (FK to merchant_master)
- **Category:** category (VARCHAR 100)
- **Source:** source (ENUM: ocr, sms, csv, manual)
- **ML Data:** confidence (JSONB), anomaly_score (FLOAT)
- **Blockchain:** blockchain_hash (VARCHAR 64), ipfs_cid (VARCHAR 100)
- **Timestamps:** created_at, updated_at
- **Indexes:**
  - (user_id, date) composite - Most common query pattern
  - (user_id, category) composite - Budget calculations
  - anomaly_score WHERE > 0.5 - Partial index for fraud detection
  - merchant_id - Foreign key performance

**3. merchant_master table**
- **Purpose:** Canonical merchant names for Person 2's matching
- **Key Fields:** id (UUID), canonical_name (VARCHAR unique), category, country
- **Aliases:** aliases (TEXT[]) - Array for fuzzy matching
- **Metadata:** tags (JSONB) - Flexible merchant properties
- **Special Index:** GIN index on aliases array for fast containment queries
- **Why Array:** PostgreSQL's GIN index makes `aliases @> ARRAY['value']` super fast

**4. budgets table**
- **Purpose:** User budget limits per category
- **Key Fields:** id (UUID), user_id (FK), category, amount (NUMERIC)
- **Period:** start_date, end_date (DATE)
- **Indexes:** (user_id, category) composite - Unique constraint, fast lookups

**5. recurrences table** (Subscriptions)
- **Purpose:** Detected recurring charges for Person 2
- **Key Fields:** id (UUID), user_id (FK), merchant_id (FK)
- **Pattern:** amount_mean (NUMERIC), period_days (INTEGER)
- **Prediction:** next_expected_date (DATE), confidence (FLOAT)
- **Indexes:** (user_id, merchant_id) composite, next_expected_date

**6. portfolio_holdings table**
- **Purpose:** User investments (stocks, crypto) for Person 3
- **Key Fields:** id (UUID), user_id (FK), asset_type (ENUM)
- **Asset:** identifier (VARCHAR) - Ticker or contract address
- **Position:** units (NUMERIC 20,8), cost_basis, last_valuation
- **Timestamp:** updated_at (for staleness detection)
- **Indexes:** (user_id, asset_type), identifier

**7. user_corrections table** (Active Learning)
- **Purpose:** Track user's category corrections for Person 2's ML
- **Key Fields:** id (UUID), transaction_id (FK), old_category, new_category
- **Optional:** reason (TEXT) - Why user corrected
- **Timestamp:** created_at (for training data freshness)
- **Indexes:** transaction_id, created_at DESC

**8. embeddings table** (Vector Search)
- **Purpose:** Store transaction embeddings for Person 2's semantic search
- **Key Fields:** id (UUID), transaction_id (FK), vector (FLOAT[384])
- **Alternative:** Use pgvector extension if available (faster similarity)
- **Indexes:** transaction_id, optional pgvector index

**9. merkle_batches table** (Blockchain)
- **Purpose:** Track Person 3's nightly anchoring batches
- **Key Fields:** id (UUID), batch_date (DATE), root_hash (VARCHAR 64)
- **Metadata:** transaction_count (INTEGER), blockchain_tx_id (VARCHAR)
- **Timestamp:** created_at
- **Indexes:** batch_date (unique), blockchain_tx_id

**10. sessions table** (Optional - if using DB sessions)
- **Purpose:** Store active user sessions across devices
- **Key Fields:** id (UUID), user_id (FK), token_hash (VARCHAR), device_info (JSONB)
- **Lifecycle:** last_active (TIMESTAMP), expires_at (TIMESTAMP)
- **Indexes:** user_id, token_hash (unique), expires_at (for cleanup)

**SQLAlchemy Model Tips:**
- Use `declarative_base()` for model inheritance
- Define relationships with `relationship()` for easy joins
- Use `back_populates` for bidirectional relationships
- Implement `__repr__` for debugging
- Add `__table_args__` for composite indexes

**Enum Types:**
- Create Python enums for transaction_source, asset_type
- Use SQLAlchemy Enum type for type safety
- **Why:** Prevents invalid values, better error messages

---

## PHASE 3: ALEMBIC MIGRATIONS (Days 7-8)

### What To Do
Set up Alembic for database version control

### How To Do It Efficiently

**RESEARCH SEPARATELY: Alembic Best Practices**
- Migration file naming conventions
- Rollback strategies
- Production migration workflows
- Data migrations vs schema migrations

**Alembic Initialization:**
1. Run `alembic init alembic` (creates alembic/ folder)
2. Configure `alembic.ini` - Set sqlalchemy.url or use env
3. Edit `alembic/env.py` - Import models, set target_metadata

**Migration Strategy:**
- **Schema migrations:** Auto-generate from model changes
- **Data migrations:** Manual scripts for transforming data
- **Indexes:** Separate migration after table creation (faster imports)

**Initial Migration:**
- Create first migration: `alembic revision --autogenerate -m "Initial schema"`
- Review generated migration (auto-gen isn't perfect)
- Add any custom logic (e.g., seed merchant data)
- Test rollback: `alembic downgrade -1`
- Apply: `alembic upgrade head`

**Seed Data Script:**
- Create `alembic/seeds/merchants.py`
- Populate merchant_master with 100+ common merchants
- Each merchant: canonical_name, category, aliases array
- Example: {"canonical_name": "Swiggy", "category": "Food & Dining", "aliases": ["SWGY", "Swiggy", "Swiggy Bangalore"]}
- Load from CSV or JSON for maintainability

**Migration Workflow for Team:**
1. Person A makes model changes
2. Generate migration: `alembic revision --autogenerate`
3. Commit migration file to Git
4. Team pulls, runs `alembic upgrade head`
5. **Everyone stays in sync**

---

## PHASE 4: AUTHENTICATION SYSTEM (Days 9-11)

### What To Do
Implement secure JWT-based authentication with access and refresh tokens

### How To Do It Efficiently

**RESEARCH SEPARATELY: JWT Security**
- Token expiration best practices
- Refresh token rotation
- Token revocation strategies (blacklist vs whitelist)
- Password hashing benchmarks (bcrypt rounds)

**Authentication Flow Design:**
```
Registration: Email + Password → Validate → Hash Password → Create User → Return Tokens
Login: Email + Password → Verify → Generate Tokens → Return
Refresh: Refresh Token → Validate → Generate New Access Token → Return
Protected Routes: Access Token → Validate → Extract User → Proceed
```

**Password Hashing:**
- **Library:** bcrypt (industry standard, slow by design)
- **Rounds:** 12 (good balance, ~300ms on modern CPU)
- **Why Not More:** 14+ rounds too slow for user experience
- **Never store plain passwords** - only hashed

**JWT Token Structure:**
- **Access Token:** Short-lived (1 hour), contains user_id in `sub` claim
- **Refresh Token:** Long-lived (30 days), allows getting new access token
- **Payload:** {"sub": user_id, "exp": expiration, "type": "access"}
- **Signature:** HMAC-SHA256 with secret key

**Token Generation Function:**
- Input: User ID, token type (access/refresh)
- Calculate expiration: current time + configured duration
- Create payload with sub, exp, type claims
- Encode with JWT library using secret and algorithm
- Return signed token string

**Token Validation:**
- Decode token with secret and algorithm
- Check expiration (library does this automatically)
- Verify type claim matches expected (access vs refresh)
- Extract user_id from sub claim
- Query database for user
- **Cache user lookup in Redis** (5 min TTL) - avoids DB hit every request

**Dependency Injection Pattern:**
- Create `get_current_user` dependency
- Accepts Authorization header: "Bearer {token}"
- Extract token, validate, return User object
- Use in route: `current_user: User = Depends(get_current_user)`
- **FastAPI caches dependency results** - same user object used throughout request

**API Endpoints:**
1. **POST /auth/register**
   - Input: email, password, full_name, phone (optional)
   - Validation: Email format, password strength (8+ chars), unique email
   - Process: Hash password, create user, generate tokens
   - Return: {access_token, refresh_token, token_type: "bearer"}

2. **POST /auth/login**
   - Input: email, password
   - Process: Find user, verify password, generate tokens
   - Error: 401 if invalid credentials
   - Return: {access_token, refresh_token}

3. **POST /auth/refresh**
   - Input: refresh_token (in body or header)
   - Process: Validate refresh token, generate new access token
   - Return: {access_token, token_type: "bearer"}
   - **Optional:** Rotate refresh token (more secure)

4. **GET /auth/me**
   - Headers: Authorization: Bearer {access_token}
   - Process: Validate token, return user profile
   - Return: {id, email, full_name, is_verified, wallet_addresses}

**Security Enhancements:**
- **Rate Limiting:** Max 5 login attempts per IP per 15 minutes
- **Password Requirements:** Enforce min length, optional complexity
- **Token Revocation:** Store refresh tokens in Redis with TTL, delete on logout
- **HTTPS Only:** In production, enforce secure token transmission

---

## PHASE 5: TRANSACTION APIS (Days 12-14)

### What To Do
Build APIs for listing, searching, and correcting transactions

### How To Do It Efficiently

**API Design Philosophy:**
- **Filtering:** Query parameters for flexibility
- **Pagination:** Limit + offset pattern (later: cursor-based)
- **Sorting:** Default by date DESC (newest first)
- **Performance:** Indexed queries, avoid N+1 problems

**1. GET /transactions**
- **Purpose:** List user's transactions with filters
- **Query Params:**
  - `since` (date): Filter transactions >= this date
  - `until` (date): Filter transactions <= this date
  - `category` (string): Filter by category
  - `source` (enum): Filter by ocr/sms/csv/manual
  - `min_amount`, `max_amount` (float): Amount range
  - `limit` (int, default 50, max 100): Results per page
  - `offset` (int, default 0): Pagination offset
- **Query Optimization:**
  - Build WHERE clauses dynamically based on provided params
  - Use composite index (user_id, date) - covers most queries
  - Avoid OR conditions (bad for index usage)
- **Return:** Array of transaction objects with merchant info (join)

**2. GET /transactions/{transaction_id}**
- **Purpose:** Get single transaction details
- **Validation:** Ensure transaction belongs to authenticated user
- **Return:** Full transaction object including receipt URL if exists
- **Error:** 404 if not found or not user's transaction

**3. POST /transactions/correct**
- **Purpose:** User corrects miscategorized transaction (for Person 2's ML)
- **Input:** transaction_id, new_category, optional reason
- **Process:**
  1. Fetch transaction, validate ownership
  2. Store correction in user_corrections table (old → new)
  3. Update transaction.category to new value
  4. **Trigger:** Person 2's retraining if 500+ corrections accumulated
- **Return:** {message: "Corrected", old_category, new_category}
- **Why Important:** Powers active learning loop

**4. POST /transactions (Manual Entry)**
- **Purpose:** User manually adds transaction
- **Input:** amount, date, merchant_raw, category, optional note
- **Process:**
  1. Create transaction with source="manual"
  2. Set high confidence (user entered)
  3. Person 3 hashes for blockchain
- **Return:** Created transaction object

**5. DELETE /transactions/{transaction_id}**
- **Purpose:** Soft delete (mark as deleted, don't actually remove)
- **Implementation:** Add `is_deleted` boolean column
- **Query Filtering:** Always filter `WHERE is_deleted = FALSE`
- **Why Soft Delete:** Preserves audit trail, can restore

**Query Performance Tips:**
- **Use `select_from().join()` for eager loading** - avoids N+1
- **Apply filters before joins** - reduces rows to join
- **Count queries separate from data queries** - faster
- **Cache frequent queries** in Redis (e.g., last 50 transactions per user)

**Response Schema Design:**
- Create Pydantic models for responses
- Include nested merchant data: {id, canonical_name, category}
- Format amounts: 2 decimal places
- Format dates: ISO 8601 (YYYY-MM-DD)
- **Exclude sensitive fields:** Don't return blockchain_hash to regular users

---

## PHASE 6: BUDGET APIS (Days 15-16)

### What To Do
Build budget management with real-time spending calculations

### How To Do It Efficiently

**1. GET /budgets**
- **Purpose:** List all budgets with current spending
- **Complex Query:**
  1. Fetch all budgets for user
  2. For each budget: SUM transactions where category matches and date in range
  3. Calculate: spent, remaining, percentage
- **Optimization:** Single query with LEFT JOIN + GROUP BY
- **Return:** Array of {budget_id, category, amount, spent, remaining, percentage, start_date, end_date}

**SQL Optimization:**
```
SELECT 
  b.id, b.category, b.amount,
  COALESCE(SUM(t.amount), 0) as spent
FROM budgets b
LEFT JOIN transactions t ON 
  t.user_id = b.user_id AND
  t.category = b.category AND
  t.date BETWEEN b.start_date AND b.end_date
WHERE b.user_id = :user_id
GROUP BY b.id
```
- **Why Efficient:** Single query instead of N+1, uses indexes

**2. POST /budgets**
- **Purpose:** Create or update budget
- **Input:** category, amount, start_date, end_date
- **Logic:**
  - Check if budget exists for user + category
  - If exists: Update amount and dates
  - If not: Create new budget
- **Upsert Pattern:** Use ON CONFLICT (PostgreSQL) or check-then-insert
- **Validation:** amount > 0, end_date > start_date

**3. GET /budgets/status**
- **Purpose:** Quick overview of budget health
- **Return:** 
  - Total budgeted across all categories
  - Total spent
  - Categories over budget (array)
  - Categories near limit (>80%)
- **Use Case:** Dashboard summary widget

**Real-Time Budget Alerts:**
- When transaction created (via webhook from Person 2):
  1. Check if category has budget
  2. Calculate new spending total
  3. If >= 80%, trigger alert via WebSocket
  4. If >= 100%, trigger critical alert
- **Implementation:** Call alert function after transaction insert

**Budget Suggestions (Optional):**
- Analyze last 3 months spending per category
- Suggest budget = average * 1.1 (10% buffer)
- Return as part of budget creation flow

---

## PHASE 7: MERCHANT APIS (Days 17)

### What To Do
Build merchant search for Person 2's fuzzy matching

### How To Do It Efficiently

**GET /merchants/search**
- **Purpose:** Search merchants by name or alias (Person 2 uses this)
- **Query Param:** `q` (search term, min 2 chars)
- **Search Strategy:**
  1. Exact match in canonical_name (fastest)
  2. Containment search in aliases array
  3. LIKE search for partial matches
- **Query:**
```sql
SELECT * FROM merchant_master
WHERE 
  canonical_name ILIKE :query
  OR :query = ANY(aliases)
ORDER BY canonical_name
LIMIT 20
```
- **Performance:** GIN index on aliases makes array search fast
- **Return:** Array of {id, canonical_name, category, aliases}

**POST /merchants (Admin Only)**
- **Purpose:** Add new merchant to master table
- **Input:** canonical_name, category, aliases array, tags
- **Validation:** Unique canonical_name
- **Access Control:** Check if user is admin (add is_admin to users table)
- **Use Case:** Person 2 discovers new merchant, adds via admin panel

**Merchant Bulk Import (Script)**
- Create CLI command: `python manage.py import_merchants data/merchants.csv`
- Read CSV with canonical_name, category, aliases
- Batch insert to database
- **Run once:** Seed initial 500+ merchants

---

## PHASE 8: WEBSOCKET REAL-TIME (Days 18-19)

### What To Do
Implement WebSocket server for real-time updates to Person 4's frontend

### How To Do It Efficiently

**RESEARCH SEPARATELY: WebSocket Scaling**
- Connection management patterns
- Redis pub/sub for multi-instance deployments
- Reconnection strategies
- Message queuing

**WebSocket Connection Flow:**
```
1. Frontend connects: ws://api.example.com/ws?token={jwt}
2. Backend validates JWT from query param
3. Store connection in ConnectionManager (user_id → websocket mapping)
4. Send connected confirmation: {"type": "connected"}
5. Listen for incoming messages (ping/pong for keepalive)
6. On disconnect: Remove from ConnectionManager
```

**Connection Manager Class:**
- **Data Structure:** Dict[user_id, WebSocket]
- **Methods:**
  - `connect(user_id, websocket)` - Add connection
  - `disconnect(user_id)` - Remove connection
  - `send_personal_message(user_id, message)` - Send to specific user
  - `broadcast(message)` - Send to all (future: rooms)

**Message Types to Send:**
1. **transaction_update:** New transaction processed
2. **budget_alert:** Spending reached 80%+ of budget
3. **anomaly_alert:** Suspicious transaction detected
4. **subscription_reminder:** Upcoming recurring charge

**Integration Points:**
- **Person 2 calls:** `await notify_transaction_update(user_id, txn_data)`
- **Budget check calls:** `await notify_budget_alert(user_id, budget_data)`
- **Person 2's anomaly detector calls:** `await notify_anomaly(user_id, txn_data)`

**Scaling Consideration:**
- **Single Instance:** In-memory ConnectionManager works
- **Multiple Instances:** Use Redis pub/sub
  - Connections on different servers
  - Publish to Redis channel
  - All servers listen, forward to connected clients
- **When to scale:** >1000 concurrent connections

**Error Handling:**
- Connection drops: Client reconnects automatically
- Invalid token: Close connection with code 1008
- Send failures: Log error, don't crash server

---

## PHASE 9: CELERY SETUP (Days 20-21)

### What To Do
Configure Celery for Person 2's background tasks (OCR, ML)

### How To Do It Efficiently

**RESEARCH SEPARATELY: Celery Best Practices**
- Task routing strategies
- Result backend options (Redis vs Database)
- Monitoring tools (Flower, Celery Events)
- Retry logic and error handling

**Celery Configuration:**
- **Broker:** Redis (fast message queue)
- **Result Backend:** Redis (store task results for 1 hour)
- **Serializer:** JSON (safer than pickle)
- **Task Acks:** Late (ack after task completes, not on receive)
- **Prefetch:** 4 (worker prefetches 4 tasks)

**Celery App Initialization:**
- Create `app/workers/celery_app.py`
- Configure broker and result backend URLs
- Set timezone and enable UTC
- Import all task modules (so Celery discovers them)

**Task Registration (Person 2 Handles):**
- Person 2 creates tasks in `app/workers/tasks.py`
- You just ensure Celery runs and discovers tasks
- Tasks: `@shared_task` decorator

**Running Celery Worker:**
- Command: `celery -A app.workers.celery_app worker --loglevel=info`
- **Production:** Use systemd service or Docker container
- **Concurrency:** `--concurrency=4` (adjust based on CPU cores)
- **Worker Types:**
  - Default: Prefork (multi-process, CPU-bound tasks)
  - Gevent: Async (I/O-bound tasks) - use for API calls

**Monitoring:**
- **Flower:** Web-based Celery monitoring
- Install: `pip install flower`
- Run: `celery -A app.workers.celery_app flower`
- Access: http://localhost:5555
- Shows: Task status, worker health, task history

**Task Patterns for Person 2:**
- **OCR Task:** Receives image path, processes, returns transaction draft
- **ML Classification:** Receives transaction data, returns category + confidence
- **Batch Tasks:** Nightly jobs (subscription detection, blockchain anchoring)

---

## PHASE 10: DOCKER & DEPLOYMENT (Days 22-24)

### What To Do
Create Docker Compose stack for local development and production deployment

### How To Do It Efficiently

**RESEARCH SEPARATELY: Docker Production**
- Multi-stage builds for smaller images
- Security: Non-root user, minimal base image
- Health checks and restart policies
- Secrets management (Docker secrets vs env vars)

**Docker Compose Services:**
1. **postgres** - Database
2. **redis** - Cache and Celery broker
3. **backend** - FastAPI application
4. **celery_worker** - Background task processor
5. **flower** (optional) - Celery monitoring

**Postgres Service:**
- Image: `postgres:15-alpine` (small, secure)
- Environment: POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB
- Volume: `postgres_data:/var/lib/postgresql/data` (persist data)
- Port: 5432:5432
- Health check: `pg_isready` command
- **Why Health Check:** Backend waits until DB ready before starting

**Redis Service:**
- Image: `redis:7-alpine`
- Volume: `redis_data:/data` (persist if needed)
- Port: 6379:6379
- Health check: `redis-cli ping`

**Backend Service:**
- Build: Dockerfile in current directory
- Command: `uvicorn app.main:app --host 0.0.0.0 --port 8000`
- Ports: 8000:8000
- Environment: DATABASE_URL, REDIS_URL, JWT_SECRET (from .env)
- Depends on: postgres, redis (wait for health checks)
- Volumes: `.:/app` (mount code for hot reload in dev)
- **Production:** Remove volume mount, use built image

**Celery Worker Service:**
- Same build as backend
- Command: `celery -A app.workers.celery_app worker --loglevel=info`
- Environment: Same as backend
- Depends on: postgres, redis
- No ports exposed (internal service)

**Dockerfile Strategy:**
- **Base image:** `python:3.11-slim` (smaller than full Python image)
- **Working directory:** `/app`
- **Copy:** requirements.txt first (Docker caching)
- **Install:** pip install (cached if requirements unchanged)
- **Copy:** Application code
- **User:** Create non-root user, switch to it
- **Command:** Defined in docker-compose (flexibility)

**Multi-Stage Build (Production):**
- **Stage 1 - Builder:** Install dependencies
- **Stage 2 - Runtime:** Copy only installed packages and code
- **Result:** 50% smaller image, faster deployments

**Environment Variables:**
- **Development:** .env file (not committed)
- **Production:** Set via hosting platform (Render, AWS, etc.)
- **Secrets:** Use platform secret management (not plain env vars)

**Running the Stack:**
- `docker-compose up -d` - Start all services background
- `docker-compose logs -f backend` - Follow backend logs
- `docker-compose down` - Stop all services
- `docker-compose up --build` - Rebuild after Dockerfile changes

**Database Initialization:**
- On first run: Migrations don't exist yet
- Add to backend entrypoint script:
```
Wait for Postgres → Run alembic upgrade head → Start uvicorn
```
- **Production:** Separate migration container before deploy

---

## DELIVERABLES CHECKLIST

### Database & Migrations
- [ ] 10+ tables created with proper fields
- [ ] All foreign keys defined with ON DELETE policies
- [ ] Indexes on frequent query patterns (7+ indexes minimum)
- [ ] GIN indexes on JSONB and arrays
- [ ] Alembic migrations working (up and down)
- [ ] Seed data script for merchants (100+ merchants)

### Authentication
- [ ] Password hashing with bcrypt (12 rounds)
- [ ] JWT access tokens (1 hour expiry)
- [ ] JWT refresh tokens (30 days expiry)
- [ ] POST /auth/register endpoint
- [ ] POST /auth/login endpoint
- [ ] POST /auth/refresh endpoint
- [ ] GET /auth/me endpoint
- [ ] `get_current_user` dependency working
- [ ] Token validation with proper errors

### Transaction APIs
- [ ] GET /transactions with filtering (6+ query params)
- [ ] GET /transactions/{id} single fetch
- [ ] POST /transactions/correct for user feedback
- [ ] POST /transactions manual entry
- [ ] DELETE /transactions soft delete
- [ ] Pagination working (limit + offset)
- [ ] Query optimization with joins

### Budget APIs
- [ ] GET /budgets with spending calculation
- ## PHASE 6: BUDGET APIS (Days 15-16) - CONTINUED

### Remaining Budget Features

**What's Left To Build:**
- GET /budgets/status endpoint
- POST /budgets/{id}/renew endpoint
- Budget alert integration with WebSocket
- Historical budget performance tracking

### Step 6.3: Budget Status Endpoint

**What To Do:**
Create dashboard overview API that shows budget health across all categories

**How To Do It Efficiently:**

**Data To Aggregate:**
- Total budgeted: Sum of all active budget amounts
- Total spent: Sum of all transactions within budget periods
- Overall percentage: (total_spent / total_budgeted) * 100
- Over budget categories: WHERE spent > budget amount
- At-risk categories: WHERE spent/budget > 0.8
- Healthy categories: Everything else

**Query Optimization Strategy:**
- Use CTE (Common Table Expression) for better readability
- Single query with CASE statements for status classification
- LEFT JOIN budgets to transactions on user_id + category + date range
- GROUP BY category to aggregate spending
- Use COALESCE to handle null sums (no spending = 0)
- **Why CTE:** More maintainable than nested subqueries, same performance

**Performance Considerations:**
- Query uses existing composite indexes (user_id, category) and (user_id, date)
- Avoid N+1 problem - one query returns everything
- Result caching in Redis (5 min TTL) for dashboard calls
- **Estimated Query Time:** <100ms for users with 1000+ transactions

**Response Structure Design:**
- Top-level summary: total_budgeted, total_spent, percentage
- Counts: over_budget_count, at_risk_count, healthy_count
- Detailed arrays: over_budget_categories, at_risk_categories
- Each category object: name, budget, spent, overage/remaining, percentage

### Step 6.4: Budget Renewal Endpoint

**What To Do:**
Allow users to renew budgets for the next period (monthly, quarterly)

**How To Do It Efficiently:**

**Renewal Logic:**
1. Fetch budget by ID, validate ownership
2. Calculate period duration (end_date - start_date)
3. Set new start_date = old end_date + 1 day
4. Set new end_date = new start_date + period duration
5. Keep budget amount same (user can edit separately)
6. Update record in database

**Auto-Renewal Consideration:**
- Add `auto_renew` boolean column to budgets table
- Person 2's Celery task checks nightly for expiring budgets
- If auto_renew = TRUE and end_date <= today, call renewal logic
- **Your Responsibility:** Provide the renewal function, Person 2 schedules it

**Edge Cases To Handle:**
- Budget already renewed (check if start_date > current end_date)
- Budget expired months ago (ask user to create new one)
- End date in future (can't renew yet)

### Step 6.5: Budget Alert Integration

**What To Do:**
Integrate budget checking with WebSocket to send real-time alerts

**How To Do It Efficiently:**

**Alert Trigger Points:**
- After every transaction insert/update
- Call budget alert function with user_id and category
- Check spending against budget for that category
- Send WebSocket message if threshold crossed

**Alert Thresholds:**
- **80% spent:** Warning alert (yellow notification)
- **90% spent:** Critical warning (orange notification)
- **100% spent:** Over budget alert (red notification)
- **110% spent:** Severe overspending (dark red notification)

**Alert Function Design:**
- Input: user_id (UUID), category (string), db session
- Query: Fetch budget for user + category
- If no budget exists: return early (nothing to alert)
- Calculate: Sum transactions in budget period for that category
- Calculate: percentage = (spent / budget) * 100
- Conditional: If percentage >= threshold, trigger WebSocket message
- **Async Call:** Use `await notify_budget_alert(user_id, alert_data)`

**WebSocket Message Format:**
- type: "budget_warning", "budget_exceeded", "budget_critical"
- category: The budget category name
- budget: The budget amount
- spent: Current spending
- percentage: How much of budget used
- remaining: Budget - spent (negative if over)

**Integration Point with Person 2:**
- Person 2's ingestion pipeline calls this after creating transaction
- You provide: `check_budget_alerts(user_id, category, db)` function
- Function is imported in Person 2's transaction creation module

**Rate Limiting Alerts:**
- Don't spam user with alerts
- Store last_alert_time in Redis per user per category
- Only send if last alert was > 1 hour ago
- **Exception:** Always alert at 100% (over budget)

### Step 6.6: Budget Suggestions (Optional)

**What To Do:**
Suggest budget amounts based on historical spending

**How To Do It Efficiently:**

**Suggestion Algorithm:**
- Look back 90 days of spending in specific category
- Calculate average monthly spending
- Add 10% buffer for safety
- Formula: suggested = (avg_monthly * 1.1)

**When To Show Suggestions:**
- During budget creation if category has no existing budget
- In budget list response as `suggested_amount` field
- User can accept suggestion or set custom amount

**Query Strategy:**
- Date filter: created_at >= (today - 90 days)
- Category filter: category = target_category
- Aggregation: AVG(amount) grouped by month
- Handle: Users with < 3 months data (suggest based on available data)

**Edge Cases:**
- No historical data: Don't suggest (return null)
- Very low spending: Set minimum suggested budget (e.g., $100)
- Very high spending: Flag as potential error (>$10,000/month)

---

## PHASE 7: MERCHANT APIS (Days 17) - CONTINUED

### Advanced Merchant Features

**Remaining Deliverables:**
- Enhanced merchant search with fuzzy matching
- POST /merchants (admin endpoint)
- Merchant bulk import script
- Merchant category statistics

### Step 7.3: Enhanced Merchant Search

**What To Do:**
Improve search to handle typos and variations in merchant names

**How To Do It Efficiently:**

**RESEARCH SEPARATELY: PostgreSQL Full-Text Search**
- pg_trgm extension for trigram similarity
- Levenshtein distance for typo tolerance
- GIN indexes for fast text search
- Performance: trgm vs LIKE vs full-text search

**Search Strategy (Multi-Tier):**
1. **Tier 1 - Exact Match:** canonical_name = query (fastest)
2. **Tier 2 - Alias Match:** query in aliases array (GIN index)
3. **Tier 3 - Prefix Match:** canonical_name LIKE 'query%'
4. **Tier 4 - Fuzzy Match:** similarity(canonical_name, query) > 0.3

**Relevance Scoring:**
- Exact match: score = 1.0
- Alias match: score = 0.9
- Prefix match: score = 0.7
- Fuzzy match: score = similarity value (0.3 to 1.0)
- Sort results by score descending

**PostgreSQL Extension Setup:**
- Run in migration: CREATE EXTENSION IF NOT EXISTS pg_trgm
- Create GIN index on canonical_name for trigram ops
- Create GIN index on aliases array (already done)

**Query Optimization:**
- Use UNION ALL to combine all tiers (not UNION - slower)
- Limit each tier before union (5 results per tier)
- Overall limit of 20 results
- **Performance:** <50ms for fuzzy search with 100k+ merchants

**Category Filtering:**
- Optional query param: `category`
- If provided: Add WHERE category = :category to all tiers
- Use category index for fast filtering

### Step 7.4: Admin Merchant Creation

**What To Do:**
Create admin-only endpoint to add new merchants to master table

**How To Do It Efficiently:**

**Admin Authentication:**
- Add `is_admin` boolean column to users table
- Create `require_admin` dependency function
- Check if current_user.is_admin == True
- Raise 403 Forbidden if not admin

**Input Validation:**
- canonical_name: Required, 3-255 chars, unique
- category: Required, must match predefined categories
- aliases: Optional array, 0-50 items
- country: Optional, 2-char ISO code
- tags: Optional JSONB object

**Duplicate Prevention:**
- Before insert: Check if canonical_name already exists (case-insensitive)
- Check if any alias matches existing merchant's canonical_name
- Suggest existing merchant if near-match found (similarity > 0.8)

**Normalization:**
- Trim whitespace from canonical_name and aliases
- Convert canonical_name to title case
- Remove duplicate aliases
- Sort aliases alphabetically

**Use Cases:**
- Person 2 discovers new merchant during ingestion
- Admin adds via admin panel (Person 4 builds UI)
- Bulk import script (see next section)

### Step 7.5: Merchant Bulk Import

**What To Do:**
Create CLI script to import hundreds of merchants from CSV

**How To Do It Efficiently:**

**Script Location:**
- Create `scripts/import_merchants.py`
- Not in main app folder (utility script)
- Can be run via: `python scripts/import_merchants.py data/merchants.csv`

**CSV Format Expected:**
- Columns: canonical_name, category, aliases, country, tags
- aliases: Pipe-separated string ("Swiggy|SWGY|Swiggy Bangalore")
- tags: JSON string ('{"type": "restaurant", "cuisine": "indian"}')
- country: Two-letter code

**Import Strategy:**
- Read CSV with pandas or Python csv module
- Validate each row (check required fields, valid category)
- Check for duplicates within CSV itself
- Batch insert using bulk operations (50-100 at a time)
- **Why Batch:** Faster than one-by-one, but not overwhelming

**Error Handling:**
- Skip invalid rows, log error
- Continue with valid rows
- Print summary: X imported, Y skipped, Z errors
- Save error log to `import_errors.log`

**Initial Seed Data:**
- Create `data/merchants_seed.csv` with 500+ common merchants
- Include: Major retailers, restaurants, utilities, subscriptions
- Sources: Plaid merchant data, manual research
- Run import once during initial deployment

**Ongoing Maintenance:**
- Periodic updates from Person 2's discoveries
- Community contributions (if public app)
- Manual admin additions

---

## PHASE 8: WEBSOCKET REAL-TIME (Days 18-19) - CONTINUED

### Advanced WebSocket Features

**Remaining Deliverables:**
- Connection state management
- Redis pub/sub for scaling
- Message types and routing
- Reconnection handling

### Step 8.3: Connection State Management

**What To Do:**
Track active WebSocket connections and handle disconnections gracefully

**How To Do It Efficiently:**

**Connection Manager Design:**
- In-memory dictionary: {user_id: WebSocket}
- Methods: connect(user_id, ws), disconnect(user_id), send(user_id, msg)
- Thread-safe operations (use async locks if concurrent)

**Connection Lifecycle:**
1. **Connect:** Client sends token in query param or first message
2. **Validate:** Decode JWT, extract user_id, check if valid
3. **Store:** Add to active_connections dict
4. **Confirm:** Send {"type": "connected", "user_id": "..."}
5. **Listen:** Keep connection open, handle ping/pong
6. **Disconnect:** Remove from dict, log disconnection

**Token Validation:**
- Accept token in WebSocket query: `ws://api/ws?token=jwt_here`
- Decode using same JWT secret as REST APIs
- Validate token type is "access" (not refresh)
- Validate expiration (if expired, close connection)
- **Security:** Close connection with code 1008 if invalid token

**Heartbeat Mechanism:**
- Client sends ping every 30 seconds
- Server responds with pong
- If no ping received for 60 seconds: assume dead, disconnect
- **Why:** Detect dead connections, free up resources

**Error Handling:**
- Invalid token: Close with 1008 (policy violation)
- Server error: Close with 1011 (internal error)
- Normal close: Code 1000
- Log all disconnections with reason

### Step 8.4: Message Types and Routing

**What To Do:**
Define different message types and handle them appropriately

**How To Do It Efficiently:**

**Outgoing Message Types (Server → Client):**
1. **connected:** Sent immediately after connection
2. **transaction_update:** New transaction processed
3. **budget_alert:** Budget threshold crossed
4. **anomaly_alert:** Suspicious transaction detected (Person 2)
5. **subscription_reminder:** Upcoming recurring charge (Person 2)
6. **portfolio_update:** Investment value changed (Person 3)

**Incoming Message Types (Client → Server):**
1. **ping:** Heartbeat from client
2. **pong:** Response to server ping
3. **subscribe:** Subscribe to specific events (optional feature)
4. **unsubscribe:** Unsubscribe from events

**Message Format Standard:**
- Always JSON
- Required field: `type` (string)
- Optional field: `data` (object with message-specific payload)
- Optional field: `timestamp` (ISO 8601)

**Message Routing:**
- Parse incoming JSON
- Switch on `type` field
- Call appropriate handler function
- Send response if needed

**Example Message Flows:**

**Transaction Update:**
- Person 2 calls: `notify_transaction_update(user_id, txn_data)`
- Server constructs: {"type": "transaction_update", "data": txn_data}
- Server sends to user's WebSocket
- Frontend updates UI instantly

**Budget Alert:**
- Budget check function calls: `notify_budget_alert(user_id, alert_data)`
- Server constructs: {"type": "budget_alert", "data": alert_data}
- Server sends to user's WebSocket
- Frontend shows notification banner

### Step 8.5: Redis Pub/Sub for Scaling

**What To Do:**
Scale WebSocket to handle multiple backend instances

**How To Do It Efficiently:**

**RESEARCH SEPARATELY: Redis Pub/Sub Patterns**
- Channel design (per-user vs broadcast)
- Message serialization (JSON vs MessagePack)
- Connection pooling
- Failure handling

**The Scaling Problem:**
- User connects to Backend Instance A
- Person 2's worker (on Instance B) processes transaction
- Worker needs to notify user via WebSocket
- But user's WebSocket is on Instance A!

**Solution: Redis Pub/Sub**
1. User connects to Instance A, stored locally
2. Instance A subscribes to Redis channel: `user:{user_id}`
3. Person 2's worker publishes to Redis: `user:{user_id}` with message
4. Instance A receives from Redis, forwards to WebSocket
5. User receives update

**Implementation Steps:**
- Create Redis client for pub/sub (separate from cache client)
- On WebSocket connect: Subscribe to user's Redis channel
- On publish: Publish to Redis instead of direct send
- On Redis message: Forward to local WebSocket if connected
- On WebSocket disconnect: Unsubscribe from Redis channel

**Channel Naming Strategy:**
- Per-user channels: `user:{user_id}` (for personal updates)
- Broadcast channel: `broadcast` (for system announcements)
- Category channels: `alerts` (for critical notifications)

**Message Serialization:**
- Serialize to JSON before publishing to Redis
- Deserialize after receiving from Redis
- Include metadata: timestamp, source_instance, message_id

**Performance Considerations:**
- Redis pub/sub is very fast (<1ms latency)
- No message persistence (if no subscribers, message lost)
- For guaranteed delivery: Use Redis Streams instead
- **When to implement:** When deploying >1 backend instance

**Fallback for Single Instance:**
- If only 1 instance: Skip Redis, use direct send
- Check: If INSTANCE_COUNT env var = 1, use simple mode
- **Benefit:** Simpler development, no Redis overhead

### Step 8.6: Reconnection Handling

**What To Do:**
Help clients reconnect smoothly after disconnection

**How To Do It Efficiently:**

**Server-Side Reconnection Support:**
- Allow same token to reconnect multiple times
- Don't store state in WebSocket connection
- Store state in Redis with user_id as key
- On reconnect: Fetch state from Redis, send catch-up updates

**Catch-Up Messages:**
- When client reconnects, it sends last_message_id
- Server queries database for events since that ID
- Send missed events via WebSocket
- **Caveat:** Only store critical events, not everything

**Connection ID Strategy:**
- Assign connection_id when client connects
- Include in all messages
- Client stores connection_id locally
- On reconnect: Send old connection_id for continuity

**Exponential Backoff (Client Implementation):**
- Tell Person 4 to implement this in frontend
- Retry after 1s, 2s, 4s, 8s, max 30s
- Give up after 5 minutes, show "offline" indicator

**Your Responsibility:**
- Ensure server accepts reconnections gracefully
- Don't rate-limit same user reconnecting
- Clear old connection when new one establishes
- Document reconnection flow for Person 4

---

## PHASE 9: CELERY SETUP (Days 20-21) - CONTINUED

### Complete Celery Infrastructure

**Remaining Deliverables:**
- Task routing and queues
- Periodic task scheduling
- Task monitoring with Flower
- Error handling and retries

### Step 9.3: Task Routing and Queues

**What To Do:**
Separate tasks into different queues based on priority and type

**How To Do It Efficiently:**

**RESEARCH SEPARATELY: Celery Task Routing**
- Queue design patterns
- Priority queues
- Task routing strategies
- Worker specialization

**Queue Design:**
- **high_priority:** OCR processing (user waiting)
- **default:** Background tasks (ML training, email sending)
- **low_priority:** Batch jobs (nightly aggregations)
- **scheduled:** Periodic tasks (subscription detection)

**Why Multiple Queues:**
- Prevent low-priority tasks from blocking urgent ones
- Scale workers independently (more for high priority)
- Better resource allocation
- Failure isolation (queue crash doesn't affect others)

**Queue Configuration:**
- Configure in celery_app.py
- Define task_routes dict: {task_name: queue_name}
- Set default_queue for unmapped tasks
- Configure task_default_priority (0-10, 5 is normal)

**Worker Specialization:**
- Worker 1: Consumes only high_priority queue
- Worker 2: Consumes default + low_priority queues
- Worker 3: Consumes scheduled queue only
- **Scaling:** Add more workers to high_priority as needed

**Task Routing Rules:**
- Person 2's OCR tasks → high_priority
- Person 2's ML training → default
- Person 3's blockchain anchoring → scheduled (nightly)
- Email notifications → default
- Database cleanup → low_priority

**Starting Workers:**
- High priority: `celery -A app.workers.celery_app worker -Q high_priority -n worker_high`
- Default: `celery -A app.workers.celery_app worker -Q default,low_priority -n worker_default`
- Scheduled: `celery -A app.workers.celery_app worker -Q scheduled -n worker_scheduled`

### Step 9.4: Periodic Task Scheduling

**What To Do:**
Schedule tasks to run automatically at intervals (cron-like)

**How To Do It Efficiently:**

**RESEARCH SEPARATELY: Celery Beat Scheduler**
- Beat scheduler configuration
- Crontab schedules vs intervals
- Database scheduler backend
- Monitoring beat scheduler

**What Needs Scheduling:**
- **Nightly:** Person 2's subscription detection (1 AM)
- **Nightly:** Person 3's blockchain anchoring (2 AM)
- **Daily:** Budget auto-renewal check (3 AM)
- **Weekly:** Send spending summary email (Monday 8 AM)
- **Monthly:** Generate monthly reports (1st of month, 12 AM)

**Celery Beat Setup:**
- Install: `pip install celery-beat`
- Configure in celery_app.py
- Define beat_schedule dict with tasks and timing
- Run beat: `celery -A app.workers.celery_app beat`

**Schedule Types:**
- **Crontab:** Run at specific times (crontab('0', '1', '*', '*', '*'))
- **Interval:** Run every X seconds/minutes (schedule(run_every=3600))
- **Solar:** Run at sunrise/sunset (for time zone aware apps)

**Schedule Examples:**
- Every day at 1 AM: crontab(hour=1, minute=0)
- Every 30 minutes: schedule(run_every=timedelta(minutes=30))
- Every Monday at 8 AM: crontab(hour=8, minute=0, day_of_week=1)

**Database Scheduler (Production):**
- Use django-celery-beat or celery-sqlalchemy-scheduler
- Store schedules in database instead of code
- Allows dynamic schedule changes via admin panel
- **Person 4 can build UI:** Add/edit scheduled tasks

**Your Tasks to Schedule:**
- Budget renewal check
- Expired session cleanup
- Database vacuum (PostgreSQL maintenance)
- Health check ping

**Person 2's Tasks to Schedule:**
- Subscription detection
- ML model retraining
- Transaction deduplication

**Person 3's Tasks to Schedule:**
- Blockchain anchoring
- IPFS pin verification

### Step 9.5: Task Monitoring with Flower

**What To Do:**
Set up Flower web dashboard to monitor Celery workers and tasks

**How To Do It Efficiently:**

**Flower Installation:**
- Install: `pip install flower`
- Add to requirements.txt
- Add service to docker-compose.yml

**Running Flower:**
- Command: `celery -A app.workers.celery_app flower`
- Access: http://localhost:5555
- **Production:** Protect with authentication (basic auth or OAuth)

**What Flower Shows:**
- Active workers and their status
- Task history (succeeded, failed, pending)
- Task execution time graphs
- Worker resource usage (CPU, memory)
- Queue lengths
- Task arguments and results

**Alerts in Flower:**
- Configure email alerts for task failures
- Configure Slack webhooks for worker crashes
- Set thresholds for queue length (alert if >1000 pending)

**Security Considerations:**
- Don't expose Flower publicly without auth
- Use flower's basic_auth option
- Or put behind reverse proxy with authentication
- **Production:** Only accessible to admins

**Alternative: Celery Events**
- If you don't want web UI: Use celery events command
- Monitor in terminal: `celery -A app.workers.celery_app events`
- Programmatic monitoring via celery events API

### Step 9.6: Error Handling and Retries

**What To Do:**
Make tasks resilient to failures with retries and error handling

**How To Do It Efficiently:**

**Retry Strategy:**
- Configure max_retries per task (default: 3)
- Configure retry_backoff (exponential backoff)
- Configure retry_backoff_max (cap at 10 minutes)
- Configure retry_jitter (randomize to avoid thundering herd)

**When to Retry:**
- **Transient errors:** Network timeouts, database connection lost
- **Rate limits:** API rate limit errors (retry after delay)
- **Resource contention:** Database locks, file access

**When NOT to Retry:**
- **Permanent errors:** Invalid data, file not found
- **Logic errors:** Business rule violations, validation errors
- **User errors:** Invalid input, unauthorized access

**Error Handling Pattern:**
- Wrap task code in try-except
- Catch specific exceptions for different handling
- Log error details (user_id, task_id, error message)
- For transient errors: raise exception to trigger retry
- For permanent errors: log and mark task as failed

**Dead Letter Queue:**
- Tasks that fail after max retries go to dead_letter queue
- Investigate manually, fix data, retry if needed
- **Celery 5+:** Built-in support for DLQ

**Task Monitoring:**
- Use celery events to track failures
- Send alerts on repeated failures (same task type)
- Daily report of failed tasks

**Your Retry Configuration:**
- OCR tasks: max_retries=3, retry_backoff=True (network calls)
- Database tasks: max_retries=5, retry_backoff=True (locks possible)
- Email tasks: max_retries=2, retry_backoff=False (fail fast)

---

## PHASE 10: DOCKER & DEPLOYMENT (Days 22-24) - CONTINUED

### Production-Ready Docker Setup

**Remaining Deliverables:**
- Multi-stage Dockerfile optimization
- Docker Compose production config
- Health checks for all services
- Secrets management
- Deployment scripts

### Step 10.3: Multi-Stage Dockerfile

**What To Do:**
Optimize Docker image size and security with multi-stage builds

**How To Do It Efficiently:**

**RESEARCH SEPARATELY: Docker Multi-Stage Builds**
- Builder pattern benefits
- Layer caching strategies
- Security scanning tools
- Image size optimization

**Why Multi-Stage:**
- **Smaller image:** Don't include build tools in final image
- **Faster deploys:** Smaller image = faster push/pull
- **More secure:** Less attack surface (no gcc, build tools)
- **Cached layers:** Dependencies cached separately from code

**Build Stages:**
1. **Stage 1 - Base:** Base Python image with system dependencies
2. **Stage 2 - Dependencies:** Install Python packages
3. **Stage 3 - Runtime:** Copy only what's needed, run app

**Stage 1 Strategy:**
- Use python:3.11-slim as base (smaller than full Python)
- Install only runtime system dependencies
- Don't install build tools here

**Stage 2 Strategy:**
- Use python:3.11 (full image with build tools)
- Copy requirements.txt
- Install all Python dependencies
- This layer rarely changes, so it caches well

**Stage 3 Strategy:**
- Start from python:3.11-slim again (fresh, minimal)
- Copy installed packages from Stage 2
- Copy application code
- Set up non-root user
- Set CMD to run application

**Layer Caching Optimization:**
- Copy requirements.txt before code (caches deps separately)
- Copy code last (changes most frequently)
- Order matters: Least changing → Most changing

**Security Hardening:**
- Create non-root user (app_user)
- Switch to that user before CMD
- Don't run as root in container
- Scan image for vulnerabilities (use docker scan or Snyk)

**Image Size Comparison:**
- Basic build: ~1.2GB
- Multi-stage: ~300MB
- **Result:** 4x smaller, 4x faster deploys

### Step 10.4: Production Docker Compose

**What To Do:**
Create separate docker-compose for production with production settings

**How To Do It Efficiently:**

**File Structure:**
- `docker-compose.yml` - Development (hot reload, debug)
- `docker-compose.prod.yml` - Production (optimized, secure)
- Use: `docker-compose -f docker-compose.prod.yml up`

**Production Differences:**
- No volume mounts (code baked into image)
- Restart policies: restart: always
- Resource limits: CPU and memory limits
- Health checks on all services
- Logging configuration
- No debug mode
- Production-grade secrets management

**Restart Policies:**
- always: Restart container if it crashes
- on-failure: Only restart if exit code != 0
- unless-stopped: Restart unless explicitly stopped
- **Use:** always for critical services (API, workers)

**Resource Limits:**
- PostgreSQL: 2 CPU, 4GB memory
- Redis: 1 CPU, 2GB memory
- Backend: 2 CPU, 2GB memory per instance
- Celery workers: 1 CPU, 2GB memory each
- **Why:** Prevent one service from starving others

**Logging Configuration:**
- Use json-file driver (structured logs)
- Set max-size: 10m (rotate after 10MB)
- Set max-file: 3 (keep last 3 rotations)
- **Production:** Ship logs to external service (ELK, Datadog)

**Environment Variables:**
- Don't use .env file in production
- Use Docker secrets (swarm mode) or env_file
- Or inject from hosting platform (AWS, Render)

### Step 10.5: Health Checks

**What To Do:**
Add health check endpoints and Docker health checks

**How To Do It Efficiently:**

**API Health Check Endpoint:**
- Already created: GET /health
- Returns: {"status": "healthy", "database": "connected"}
- Check database connection
- Check Redis connection
- Return 200 if all healthy, 503 if any service down

**Enhanced Health Check:**
- Check: Database query (SELECT 1)
- Check: Redis ping
- Check: Celery workers active (query broker)
- Check: Disk space available
- Return: Detailed status of each component

**Docker Health Check Configuration:**
- Add healthcheck to each service in docker-compose
- Test command (e.g., pg_isready for Postgres)
- Interval: How often to check (10s)
- Timeout: Max time for check (5s)
- Retries: Failures before marked unhealthy (3)
- Start period: Grace period on startup (30s)

**Health Check Examples:**

**Postgres:**
- Test: pg_isready -U financeuser
- Interval: 10s
- Means: Check if Postgres accepting connections every 10s

**Redis:**
- Test: redis-cli ping
- Interval: 10s
- Means: Check if Redis responding every 10s

**Backend:**
- Test: curl -f http://localhost:8000/health || exit 1
- Interval: 30s
- Means: Check API responding every 30s

**Why Health Checks Matter:**
- Container orchestrators (K8s, ECS) restart unhealthy containers
- Load balancers route around unhealthy instances
- Monitoring systems alert on unhealthy status
- Prevents routing requests to dead services

**Dependencies Configuration:**
- Use depends_on with condition: service_healthy
- Backend waits for Postgres and Redis to be healthy
- Prevents startup errors from missing dependencies

### Step 10.6: Secrets Management

**What To Do:**
Securely handle sensitive credentials in production

**How To Do It Efficiently:**

**RESEARCH SEPARATELY: Docker Secrets**
- Docker Swarm secrets
- Docker Compose secrets (v3.1+)
- Kubernetes secrets
- Cloud provider secret managers

**Never Do This:**
- Hardcode secrets in Dockerfile
- Commit .env file to Git
- Pass secrets in docker-compose.yml plaintext
- Log secrets in application logs

**Development Secrets:**
- Use .env file (gitignored)
- Keep .env.example in Git (no real values)
- Team shares secrets via secure channel (1Password, LastPass)

**Production Secrets Options:**

**Option 1: Environment Variables (Simple)**
- Set via hosting platform (Render, Railway, Heroku)
- Injected at runtime, not in code
- Easy to rotate
- **Downside:** Visible in process list

**Option 2: Docker Secrets (Swarm Mode)**
- Store secrets in Docker Swarm
- Mounted as files in /run/secrets/
- Read from file instead of env var
- **Benefit:** Never in environment or logs

**Option 3: Cloud Secret Managers**
- AWS Secrets Manager, Google Secret Manager, Azure Key Vault
- Fetch secrets at application startup
- Automatically rotate secrets
- Audit log of secret access
- **Best for:** Production at scale

**Your Implementation (Start Simple):**
- Development: .env file
- Production: Environment variables from platform
- Later: Migrate to secret manager as you scale

**Secret Rotation:**
- JWT_SECRET: Rotate every 6 months
- Database password: Rotate annually
- API keys: Rotate if compromised
- **Process:** Update secret, deploy new version, old tokens expire naturally

### Step 10.7: Deployment Script

**What To Do:**
Create automated deployment script for production

**How To Do It Efficiently:**

**Script Purpose:**
- Automate deployment steps
- Reduce human error
- Ensure consistent deployments
- Roll ## DELIVERABLES CHECKLIST (CONTINUED)

### Database & Migrations (Complete These First)
- [ ] All models have proper `__repr__` methods for debugging
- [ ] Migration rollback tested (downgrade then upgrade)
- [ ] Seed merchant data includes minimum 500 entries
- [ ] Database backup strategy documented
- [ ] Index performance verified with EXPLAIN ANALYZE

### Authentication (Security Critical)
- [ ] Rate limiting on login endpoint (5 attempts per 15 min)
- [ ] Password reset flow (email with token)
- [ ] Email verification flow  
- [ ] Refresh token rotation implemented
- [ ] Token blacklist for logout (Redis-based)

### Transaction APIs (Core Feature)
- [ ] Bulk transaction upload endpoint (CSV import)
- [ ] Transaction export endpoint (CSV/JSON download)
- [ ] Transaction search with full-text (merchant + notes)
- [ ] Duplicate detection logic (same amount + date + merchant)
- [ ] Soft delete verified (deleted items don't appear in queries)

### Budget APIs (User Engagement)
- [ ] Budget status endpoint working
- [ ] Budget renewal endpoint working
- [ ] Budget comparison (this month vs last month)
- [ ] Budget recommendations based on history
- [ ] WebSocket alerts integrated and tested

### Merchant APIs (Data Quality)
- [ ] Fuzzy search with pg_trgm working
- [ ] Admin merchant creation protected
- [ ] Bulk import script completed and tested
- [ ] Merchant merge function (combine duplicates)
- [ ] Merchant usage statistics (most common per user)

### WebSocket Real-Time (User Experience)
- [ ] Connection manager handles 100+ concurrent users
- [ ] Message routing working for all message types
- [ ] Reconnection tested (disconnect/reconnect works)
- [ ] Redis pub/sub working for multi-instance
- [ ] Heartbeat mechanism prevents dead connections

### Celery Infrastructure (Background Processing)
- [ ] Task routing to correct queues verified
- [ ] Periodic tasks scheduled and running
- [ ] Flower dashboard accessible and protected
- [ ] Error handling with retries working
- [ ] Dead letter queue monitoring in place

### Docker & Deployment (DevOps)
- [ ] Multi-stage build reduces image to <350MB
- [ ] Production compose has all health checks
- [ ] Secrets managed securely (not in code)
- [ ] Deployment script automates full deploy
- [ ] Rollback procedure documented and tested

### Integration Testing
- [ ] Authentication flow end-to-end
- [ ] Transaction creation to WebSocket notification
- [ ] Budget alert triggered when limit exceeded
- [ ] Celery task completes and updates database
- [ ] API response times <500ms p95

---

## INTEGRATION HANDOFF GUIDE

### For Person 2 (Ingestion & ML)

**You Provide These Functions:**
```
Database Access:
- get_db() generator for database sessions
- Transaction model with all fields
- MerchantMaster model for lookups
- UserCorrection model for active learning data

API Endpoints to Call:
- POST /transactions (create transaction programmatically)
- GET /merchants/search?q=... (fuzzy merchant matching)
- Helper: check_budget_alerts(user_id, category, db)

Celery Infrastructure:
- celery_app for registering tasks
- Task queues: high_priority, default, scheduled
- Redis connection for caching

WebSocket Functions:
- notify_transaction_update(user_id, transaction_data)
- notify_anomaly_alert(user_id, anomaly_data)
```

**You Receive From Them:**
- Transaction data objects (amount, merchant, category, confidence)
- Merchant matching requests (raw text → canonical merchant)
- Category classification results (transaction → category)
- Anomaly scores (transaction → risk score 0-1)

**Integration Pattern:**
1. Person 2's OCR task extracts transaction from receipt
2. Calls your merchant search API to find canonical merchant
3. Calls ML classifier to get category
4. Creates Transaction record in your database
5. Calls your budget alert function
6. Calls your WebSocket notify function
7. **Result:** User sees transaction instantly in app

### For Person 3 (Blockchain)

**You Provide These Functions:**
```
Database Fields:
- Transaction.blockchain_hash (VARCHAR 64) for SHA-256
- Transaction.ipfs_cid (VARCHAR 100) for receipt CID
- MerkleBatch model for anchoring records

Database Write Access:
- Update transactions with hash/CID after anchoring
- Create MerkleBatch records for nightly batches

API Endpoints:
- GET /transactions?since=... (fetch transactions to anchor)
- Internal function: mark_transactions_anchored(tx_ids, root_hash)
```

**You Receive From Them:**
- Blockchain transaction hashes (after anchoring)
- IPFS CIDs (after uploading receipts)
- Merkle root hashes (for batch verification)
- Verification functions (user can verify transaction authenticity)

**Integration Pattern:**
1. Person 3's nightly Celery task runs at 2 AM
2. Queries your database for unanchored transactions
3. Computes Merkle tree, submits root to blockchain
4. Updates your Transaction records with blockchain_hash
5. Creates MerkleBatch record in your database
6. **Result:** Transactions are tamper-proof and verifiable

### For Person 4 (Frontend)

**You Provide These Endpoints:**
```
Documentation:
- Swagger UI at /docs (auto-generated from FastAPI)
- OpenAPI spec at /openapi.json
- Authentication guide (JWT flow)

REST APIs:
- All endpoints from Phases 3-7
- Consistent error responses (status codes + messages)
- Pagination on list endpoints (limit, offset, total)

WebSocket:
- Connection URL: ws://api/ws?token=jwt
- Message format: {type: string, data: object}
- Reconnection support (client can reconnect with same token)

CORS:
- Frontend origins whitelisted in settings
- Credentials allowed for cookies/auth headers
```

**You Receive From Them:**
- HTTP requests to REST APIs
- WebSocket connections from browser
- Transaction corrections (user fixes category)
- User preferences updates

**Integration Pattern:**
1. Person 4's React app calls POST /auth/login
2. Receives JWT access and refresh tokens
3. Stores tokens in secure storage
4. Opens WebSocket connection with token
5. Makes authenticated API calls with Bearer token
6. Receives real-time updates via WebSocket
7. **Result:** Responsive, real-time user experience

---

## OPTIMIZATION CHECKLIST

### Database Performance
- [ ] Query performance tested with 100k+ transactions per user
- [ ] Indexes verified with EXPLAIN ANALYZE
- [ ] Connection pool size optimized for load
- [ ] Long-running queries identified and optimized
- [ ] Database statistics collected (ANALYZE command)

### API Performance
- [ ] Redis caching on frequently accessed endpoints
- [ ] Response times measured (p50, p95, p99)
- [ ] N+1 query problems eliminated
- [ ] Pagination prevents large result sets
- [ ] Rate limiting prevents abuse

### Celery Performance
- [ ] Task execution times monitored
- [ ] Worker count scaled to load
- [ ] Queue lengths monitored
- [ ] Task deduplication (don't process same task twice)
- [ ] Resource usage optimized (memory leaks fixed)

### Docker Performance
- [ ] Image layers optimized for caching
- [ ] Resource limits set appropriately
- [ ] Health checks tuned (not too frequent)
- [ ] Logs rotated to prevent disk fill
- [ ] Volumes used for persistence

---

## SECURITY CHECKLIST

### Authentication Security
- [ ] Passwords hashed with bcrypt (12 rounds minimum)
- [ ] JWT secrets strong (64+ chars random)
- [ ] Tokens have proper expiration
- [ ] Refresh token rotation enabled
- [ ] Rate limiting on auth endpoints

### API Security
- [ ] All endpoints require authentication (except public ones)
- [ ] Authorization checks (user owns resource)
- [ ] Input validation on all endpoints
- [ ] SQL injection prevented (ORM usage)
- [ ] XSS prevention (output escaping)

### Infrastructure Security
- [ ] Docker containers run as non-root user
- [ ] Secrets not in code or images
- [ ] HTTPS enforced in production
- [ ] CORS properly configured
- [ ] Security headers set (HSTS, CSP, etc.)

### Database Security
- [ ] Database user has minimum privileges
- [ ] Connection encrypted (SSL/TLS)
- [ ] Backups encrypted at rest
- [ ] Audit logging enabled
- [ ] PII data handling compliant

---

## MONITORING & OBSERVABILITY

### What To Monitor
- API response times (latency)
- Error rates (5xx responses)
- Database query times
- Celery task success/failure rates
- WebSocket connection count
- Memory and CPU usage
- Disk usage

### How To Monitor

**Application Metrics:**
- Add Prometheus metrics to FastAPI
- Expose /metrics endpoint
- Track: request_count, request_duration, error_count
- Use FastAPI middleware for automatic instrumentation

**Database Metrics:**
- PostgreSQL built-in statistics (pg_stat_statements)
- Monitor: query execution time, connection count, cache hit ratio
- Use monitoring tools: pgAdmin, DataDog, or custom queries

**Celery Metrics:**
- Flower dashboard (task history, worker status)
- Celery events (programmatic monitoring)
- Monitor: task duration, failure rate, queue length

**Log Aggregation:**
- Centralize logs from all services
- Use: ELK Stack, Splunk, or cloud provider logging
- Structure logs as JSON for easier parsing
- Include: request_id, user_id, timestamp in all logs

**Alerting:**
- Alert on error rate >1% (5xx responses)
- Alert on API latency >1s p95
- Alert on database connection errors
- Alert on Celery worker down
- Alert on disk usage >80%

---

## SCALING CONSIDERATIONS

### When You Outgrow Single Instance

**Vertical Scaling (First Step):**
- Increase server resources (CPU, RAM)
- Optimize database queries
- Add Redis caching aggressively
- Optimize Docker resource limits
- **Good until:** 10k+ users, 1M+ transactions

**Horizontal Scaling (Next Step):**
- Multiple backend instances behind load balancer
- Redis pub/sub for WebSocket (already prepared)
- Database read replicas for read-heavy endpoints
- Separate Celery workers for different queues
- **Good until:** 100k+ users, 10M+ transactions

**Database Scaling:**
- Partition large tables by date
- Use materialized views for complex queries
- Consider Aurora, Citus, or other distributed Postgres
- Separate read and write databases
- **Good until:** 1M+ users, 100M+ transactions

**Celery Scaling:**
- More worker instances (horizontal scaling)
- Separate queues by task type
- Multiple beat schedulers (with locking)
- Consider: RabbitMQ instead of Redis (more features)

---

## FINAL SUCCESS CRITERIA

### Technical Excellence
✅ All APIs respond <500ms p95  
✅ Zero unhandled exceptions in production  
✅ Database queries optimized (<100ms each)  
✅ 100% test coverage on auth and core APIs  
✅ Security audit passed  
✅ Code documented (docstrings + README)  

### Team Integration
✅ Person 2 successfully creates transactions  
✅ Person 3 successfully anchors transactions  
✅ Person 4 successfully builds frontend against APIs  
✅ All integration points tested and working  
✅ API contract documented and stable  

### Production Readiness
✅ Docker stack runs in production  
✅ Monitoring and alerting configured  
✅ Backup and restore procedure tested  
✅ Scaling strategy documented  
✅ Incident response plan written  
✅ Deployment pipeline automated  

---

## COMMON PITFALLS TO AVOID

**Database Mistakes:**
- Missing indexes on foreign keys
- Not using composite indexes for multi-column queries
- Forgetting to VACUUM ANALYZE PostgreSQL
- Not planning for data growth (partitioning)

**API Design Mistakes:**
- Inconsistent error response formats
- No API versioning strategy
- Breaking changes without deprecation period
- Poor pagination (offset-based at scale)

**Celery Mistakes:**
- Not setting max_retries (infinite retry loops)
- Blocking tasks (use async or separate workers)
- No dead letter queue (lost failed tasks)
- Not monitoring queue lengths

**Docker Mistakes:**
- Running as root in containers
- Secrets in images or environment
- No resource limits (one container kills others)
- Not using health checks

**Security Mistakes:**
- Weak JWT secrets
- No rate limiting
- SQL injection vulnerabilities
- Missing input validation

---

## YOU'VE BUILT THE FOUNDATION

You've created:
- ✅ Solid database schema with proper indexes
- ✅ Secure authentication with JWT
- ✅ Complete REST API with all core features
- ✅ Real-time WebSocket for instant updates
- ✅ Background task processing with Celery
- ✅ Production-ready Docker infrastructure

Your work enables:
- Person 2 to ingest and process transactions
- Person 3 to anchor data on blockchain
- Person 4 to build amazing user interface

**Everyone depends on you. You delivered.**

---

## WHAT'S NEXT (FUTURE ENHANCEMENTS)

**Phase 2 Features** (After MVP):
- GraphQL API (in addition to REST)
- Server-Sent Events (alternative to WebSocket)
- Multi-tenancy support (company accounts)
- Advanced search with Elasticsearch
- API rate limiting per user tier
- Webhook system for external integrations

**Scaling Improvements:**
- Read replicas for database
- CDN for static assets
- Message queue (RabbitMQ or Kafka)
- Service mesh (Istio, Linkerd)
- Kubernetes deployment

**Developer Experience:**
- Auto-generated API client libraries
- Interactive API playground
- Better error messages
- Request/response examples in docs
- Postman collection

---

**YOU ARE THE FOUNDATION ARCHITECT. YOU BUILT IT SOLID. SH
